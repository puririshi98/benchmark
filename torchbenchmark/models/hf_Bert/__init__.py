
####################################
# Generated by gen_hf_generative.py#
####################################

import torch
import torch.optim as optim
import torchvision.models as models
from ...util.model import BenchmarkModel
from torchbenchmark.tasks import NLP
from .modeling import *
from datasets import load_dataset
import torch.cuda.nvtx as nvtx
import time
class Model(BenchmarkModel):
    task = NLP.LANGUAGE_MODELING

    def __init__(self, device=None, jit=False, precision='float32', batchsize=1, seqlen=128, large=False):
        super().__init__()
        self.device = device
        self.jit = jit

        torch.manual_seed(42)
        config = BertConfig(vocab_size_or_config_json_file=30522,
            max_position_embeddings=512, hidden_size=1024 if large else 768,
            intermediate_size=4096 if large else 3072,
            num_attention_heads=16 if large else 12,
            num_hidden_layers=24 if large else 12
        )
        #self.model = BertModel(config).to(device)
        self.model = BertForQuestionAnswering(config).to(device)
        if jit:
            self.model = torch.jit.script(self.model)
            assert isinstance(self.model, torch.jit.ScriptModule)
        self.eval_inputs = {
            'input_ids': torch.randint(0, config.vocab_size, (batchsize, seqlen)).to(device),
            'mask':  torch.randint(0, 2, (batchsize, seqlen)).to(device),
            'token_type_ids':torch.randint(0, 2, (batchsize, seqlen)).to(device)
        }

    def get_module(self):
        return self.model, self.eval_inputs

    def _step_eval(self, precision):
        nvtx.range_push('eval')
        output = self.model(self.eval_inputs['input_ids'], self.eval_inputs['mask'], self.eval_inputs['token_type_ids'])
        nvtx.range_pop()
    def eval(self, niter=1, precision='fp16', graphs=False, bench=False):
        niter = 8
        # with torch.autograd.profiler.emit_nvtx(record_shapes=True):
        self.model.eval()
        torch.backends.cudnn.benchmark = bench
        with torch.no_grad():
            if precision == 'fp16':
                self.model = self.model.half()
            elif precision == 'bfloat16':
                self.model=self.model.bfloat16()
            if graphs:
                s = torch.cuda.Stream()
                torch.cuda.synchronize()
                with torch.cuda.stream(s):
                    nvtx.range_push('warming up')
                    print('warming up')
                    for _ in range(5):
                        self._step_eval(precision)
                    nvtx.range_pop()
                    torch.cuda.empty_cache()
                    g = torch.cuda._Graph()
                    torch.cuda.synchronize()
                    nvtx.range_push('capturing graph')
                    print('capturing graph')
                    g.capture_begin()
                    self._step_eval(precision)
                    g.capture_end()
                    nvtx.range_pop()
                    torch.cuda.synchronize()
                nvtx.range_push('replaying')
                print('replaying')
                since=time.time()
                for _ in range(100):
                    g.replay()
                    # torch.cuda.synchronize()
                print("Average Replay Time for bert:",round(1000.0 * (time.time()-since)/100.0,5),"ms")
                nvtx.range_pop()
            else:
                torch.cuda.synchronize()
                
                for i in range(5):
                    self._step_eval(precision)
                    torch.cuda.synchronize()
                since=time.time()
                for i in range(100):
                    self._step_eval(precision)
                    torch.cuda.synchronize()
                print("Average Replay Time for bert:",round(1000.0 * (time.time()-since)/100.0,5),"ms")

    